{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ3HloS1Ok-"
      },
      "source": [
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        "    Artificial Intelligence <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 3 - Reinforcement Learning <br>\n",
        "<font color=696880 size=4>\n",
        "    Mohammad Moshtaghi - Ali Salesi - Hossein Goli\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfGdour1cNK"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IM62bqV51dy2"
      },
      "outputs": [],
      "source": [
        "student_number = '99109788'\n",
        "first_name = 'mobina'\n",
        "last_name = 'salimipanha'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3KLkyuZo0tR"
      },
      "source": [
        "# Rules\n",
        "- Make sure that all of your cells can be run perfectly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91za1kfo7uB"
      },
      "source": [
        "# Q2: Sentence Generator (100 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80phsyNDjo1"
      },
      "source": [
        "<font size=4>\n",
        "Author: Ali Salesi\n",
        "<br/>\n",
        "<font color=red>\n",
        "Please run all the cells.\n",
        "</font>\n",
        "</font>\n",
        "<br/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vBT4rxeHnM"
      },
      "source": [
        "In this assignment we implement a text generator using RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQRBpSNJ2ICr"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDg8VW5k3A4Z"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFoOb7GIRI3"
      },
      "source": [
        "First, lets download the text corpus crawled from `VOA Persian` from 2003 to 2008."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxG1ZP8Gqk2_",
        "outputId": "1fcc97a9-2c3f-4a31-bf1c-ff474787c85d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-15 08:34:38--  https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.31.128, 142.251.18.128, 142.250.153.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.31.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69708061 (66M) [text/plain]\n",
            "Saving to: ‘voa_persian.txt’\n",
            "\n",
            "voa_persian.txt     100%[===================>]  66.48M  29.4MB/s    in 2.3s    \n",
            "\n",
            "2023-05-15 08:34:41 (29.4 MB/s) - ‘voa_persian.txt’ saved [69708061/69708061]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O \"voa_persian.txt\" \"https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzh2UPwisjTG",
        "outputId": "f1e306e8-84ab-4c9e-ef21-ac65ddae2785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "488253\n"
          ]
        }
      ],
      "source": [
        "!wc -l voa_persian.txt | awk '{print $1}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDKxtmaAqvjB",
        "outputId": "6c34be01-aafa-4699-80bd-1c5b9a5b4d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پيمان صلح بين ژاپن و روسيه\n",
            "بنا به گزارشهای منتشره در توکيو، ژاپن و روسيه در زمينه يک پيمان صلح در چارچوبی گسترده توافق کرده اند که رسماً به مخاصمات جنگ دوم جهانی ميان دو کشور پايان خواهند داد.\n",
            "\n",
            "در يکی از اين گزارشها، که از سوی خبرگزاری کيودُو،انتشار يافته، گفته شده است که دو کشور برای رفع اختلافات ديرين خود بر سر چهار جزيره از جزاير زنجيره ای کوريل، بر اساس سه پيمان گذشته خود عمل خواهند کرد.\n",
            "بموجب يکی از اين پيمانها که در سال ۱۹۵۶ امضاء شده، دو تا از اين جزيره ها پس از امضاء يک پيمان صلح به ژاپن پس داده خواهد شد.\n",
            "اما بموجب پيمانی که در سال ۱۹۹۳ به امضاء رسيده، مسئله حاکميت اين چهار جزيره بايستی پيش از امضاء پيمان صلح فيصله يابد.\n",
            "هيچ يک از دو طرف نحوه استفاده از پيمان های پيشين را اعلام نکرده اند.\n",
            "\n",
            "تشکيلات فلسطينی نخستين بودجه رسمی خود را اعلام کرد\n",
            "تشکيلات فلسطينی پس از دو سال نخستين بودجه رسمی خود را اعلام کرد و قول داد برای از ميان برداشتن فساد و پاسخگوئی بيشتر به مردم تلاش کند.\n"
          ]
        }
      ],
      "source": [
        "!head voa_persian.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj9v5jUm2691"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx5YwlZar85k"
      },
      "source": [
        "Then we have to normalize and lemmatize the text so we can have a better generalization of semantics in prompt generation.\n",
        "\n",
        "We'll use `hazm` library for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwihzn7rzAC",
        "outputId": "68d75f7a-c34e-4278-83d8-5e1da0fdbeea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.3 (from hazm)\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1 (from hazm)\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394470 sha256=75c9d91f12595746c2e61b87502b79ad09206329ed92b65ae8429fb9bcdf6226\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-linux_x86_64.whl size=180369 sha256=34796b72673a13a5c5587d867c331a3b5b3e616188b8de67415d8e4701ea782d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c_qV0iYsr-lB"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "\n",
        "def normalize(line: str):\n",
        "    line = re.sub(\n",
        "        r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', line.strip())\n",
        "    line = re.sub(r'\\s+', ' ', line.strip())\n",
        "    line = normalizer.normalize(line)\n",
        "    words = word_tokenize(line)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    line = ' '.join(words)\n",
        "    return line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bJwjSKcs1eEf",
        "outputId": "1bb584fd-37a0-4e12-f200-f004c6f595d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'من خیلی خوشحال #هست و کتاب زیاد درباره یخچال قطب خواند#خوان'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "normalize('من خیلی خوشحال هستم و کتاب‌های زیادی درباره یخچال‌های قطبی خوانده‌ام.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOD9fyM3sK8K",
        "outputId": "46ccfb24-ed81-4e45-87fe-cfbab1ab7f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 488253/488253 [00:49<00:00, 9883.77it/s]\n"
          ]
        }
      ],
      "source": [
        "voa = open('voa_persian.txt')\n",
        "voa_norm = open('voa_persian_normalized.txt', 'w')\n",
        "for i, line in tqdm(enumerate(voa), total=488253):\n",
        "    voa_norm.write(normalize(line) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df4v9sYDxDT9",
        "outputId": "eabb2ce9-61db-416a-bfcf-ab8129744a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پیمان صلح بین ژاپن و روسیه\n",
            "بنا به گزارش منتشره در توکیو ژاپن و روسیه در زمینه یک پیمان صلح در چارچوب گسترده توافق کرد#کن که رسما به مخاصمات جنگ دوم جهانی میان دو کشور پایان داد#ده\n",
            "\n",
            "در یک از این گزارش که از سو خبرگزاری کیودوانتشار یافته گفت#گو که دو کشور برای رفع اختلافات دیرین خود بر سر چهار جزیره از جزایر زنجیره کوریل بر اساس سه پیمان گذشته خود عمل کرد#کن\n",
            "بموجب یک از این پیمان که در سال ۱۹۵۶ امضاء شده دو تا از این جزیره پس از امضاء یک پیمان صلح به ژاپن پس داد#ده\n",
            "اما بموجب پیمان که در سال ۱۹۹۳ به امضاء رسیده مسئله حاکمیت این چهار جزیره ایستاد#ایست پیش از امضاء پیمان صلح فیصله یافت#یاب\n",
            "هیچ یک از دو طرف نحوه استفاده از پیمان پیشین را اعلام کرد#کن\n",
            "\n",
            "تشکیلات فلسطین نخستین بودجه رسم خود را اعلام کرد#کن\n",
            "تشکیلات فلسطین پس از دو سال نخستین بودجه رسم خود را اعلام کرد#کن و قول داد برای از میان برداشتن فساد و پاسخگوئی بیشتر به مردم تلاش کند\n"
          ]
        }
      ],
      "source": [
        "!head voa_persian_normalized.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-rAN6e2tNx"
      },
      "source": [
        "### Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWdUYOwnsXKj"
      },
      "source": [
        "Now we'll use `KenLM` to train an N-gram language model. an N-gram model calculates probability of N words being together.\n",
        "\n",
        "You can read more about N-gram [here](https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058).\n",
        "\n",
        "First, let's install download and build `KenLM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtuYeg5Gq2AW",
        "outputId": "609220d4-e018-4379-a104-878c82c42878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-15 08:36:14--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491888 (480K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 480.36K   965KB/s    in 0.5s    \n",
            "\n",
            "2023-05-15 08:36:15 (965 KB/s) - written to stdout [491888/491888]\n",
            "\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.4\") \n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 51%] Built target probing_hash_table_benchmark\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 57%] Built target kenlm_filter\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 75%] Built target fragment\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 77%] Built target query\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 87%] Built target kenlm_benchmark\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 91%] Built target kenlm_builder\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 93%] Built target phrase_table_vocab\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 96%] Built target filter\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ],
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz; mkdir kenlm/build; cd kenlm/build; cmake ..; make -j2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NB9vhHY3Wkt"
      },
      "source": [
        "Now let's make a 5-gram model using "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvr9XW2btIp-",
        "outputId": "06679a81-b00a-43cd-abb9-c687331d877d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/voa_persian_normalized.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 7151282 types 105479\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:1265748 2:1062379968 3:1991962624 4:3187139840 5:4647912448\n",
            "Statistics:\n",
            "1 105479 D1=0.692798 D2=1.02059 D3+=1.36868\n",
            "2 1273831 D1=0.753634 D2=1.09875 D3+=1.3404\n",
            "3 3442840 D1=0.837136 D2=1.17748 D3+=1.39394\n",
            "4 5019073 D1=0.905517 D2=1.28916 D3+=1.43789\n",
            "5 5610872 D1=0.891831 D2=1.51472 D3+=1.61131\n",
            "Memory estimate for binary LM:\n",
            "type     MB\n",
            "probing 321 assuming -p 1.5\n",
            "probing 377 assuming -r models -p 1.5\n",
            "trie    153 without quantization\n",
            "trie     83 assuming -q 8 -b 8 quantization \n",
            "trie    135 assuming -a 22 array pointer compression\n",
            "trie     66 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10807668 kB\tVmRSS:29576 kB\tRSSMax:2062888 kB\tuser:15.6177\tsys:4.67224\tCPU:20.2899\treal:23.1901\n"
          ]
        }
      ],
      "source": [
        "!kenlm/build/bin/lmplz -o 5 <\"voa_persian_normalized.txt\"> \"voa_persian.arpa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIy7Bo8FDAO",
        "outputId": "52af5d9d-b759-41d6-fca6-642fdf65057f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\data\\\n",
            "ngram 1=105479\n",
            "ngram 2=1273831\n",
            "ngram 3=3442840\n",
            "ngram 4=5019073\n",
            "ngram 5=5610872\n",
            "\n",
            "\\1-grams:\n",
            "-6.138535\t<unk>\t0\n",
            "0\t<s>\t-1.5815679\n",
            "-2.1129756\t</s>\t0\n",
            "-3.5871809\tپیمان\t-0.50824106\n",
            "-3.304699\tصلح\t-0.560521\n",
            "-2.891446\tبین\t-0.67797303\n",
            "-3.303326\tژاپن\t-0.5074899\n",
            "-2.0037236\tو\t-0.8103652\n",
            "-3.097553\tروسیه\t-0.56382215\n",
            "-3.694238\tبنا\t-0.5076225\n",
            "-2.0797832\tبه\t-1.0190648\n",
            "-3.047614\tگزارش\t-0.6365477\n"
          ]
        }
      ],
      "source": [
        "!head -n 20 voa_persian.arpa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkcS4dFtujvo"
      },
      "source": [
        "Now lets extract the list of words and sort them using their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSR4Cw6st1G7",
        "outputId": "b6170b11-0260-4e8b-c8f7-569d76472ea9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>',\n",
              " 'در',\n",
              " 'و',\n",
              " 'به',\n",
              " 'را',\n",
              " 'که',\n",
              " 'از',\n",
              " 'با',\n",
              " '#است',\n",
              " 'بود#باش',\n",
              " 'یک',\n",
              " 'برای',\n",
              " 'این',\n",
              " 'شد#شو',\n",
              " 'گفت#گو',\n",
              " 'خود',\n",
              " 'آن',\n",
              " 'کرد#کن',\n",
              " 'روز',\n",
              " 'نیز']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "words = []\n",
        "words_started = False\n",
        "with open('voa_persian.arpa') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not words_started:\n",
        "            if line == r'\\1-grams:':\n",
        "                words_started = True\n",
        "        else:\n",
        "            if line == r'\\2-grams:':\n",
        "                words = words[:-1]\n",
        "                break\n",
        "            words.append(line.split())\n",
        "words_sorted = sorted(words, key=lambda x: x[0])\n",
        "words_total = [w[1] for w in words_sorted]\n",
        "words_total.remove('</s>')\n",
        "words_total.insert(0, '</s>')\n",
        "words_total[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxznD7kt3f_T",
        "outputId": "70d3b48c-fb12-435c-ede6-293e099d3f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.5/553.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp310-cp310-linux_x86_64.whl size=3255305 sha256=07664f65e9e2ffc2a76095e4df04d2ffbc1ca4abcb65f2c9163313d1c02fe82c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1eht83ab/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eIeI7WM13nK4"
      },
      "outputs": [],
      "source": [
        "import kenlm\n",
        "\n",
        "model = kenlm.Model('voa_persian.arpa')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RepmrJXhzmjt"
      },
      "source": [
        "Now we need a measure using our language model to measure how well our sentence fit together. Our model can measure the probability of a sentence using N-gram.\n",
        "\n",
        "This has a downside. the longer the sentence gets, the lower its' probability becomes. We don't want that. So we introduce `perplexity`. a measure which is normalized by the sentence's length. Lower perplexity means the semantics of our sentence fits better together.\n",
        "\n",
        "You can read more about perplexity [here](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n",
        "$$\n",
        "\\begin{align}\n",
        "PP(S) &= 10 ^ {-\\frac{log(P(S))}{N}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\frac{1}{P(S)}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\frac{1}{P(W_1W_2...W_N)}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{P(W_i|W_1W_2...W_{i-1})}}}\n",
        "\\end{align}\n",
        "$$\n",
        "**Note**: `KenLM` score function return log10 probability of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diKetldhDjo6"
      },
      "source": [
        "### Perplexity (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oPH4awk57yOZ"
      },
      "outputs": [],
      "source": [
        "def perplexity(sentence: str):\n",
        "    \"\"\"\n",
        "    returns the perplexity of a sentence using model.score method\n",
        "    Args:\n",
        "      sentence: string of words\n",
        "\n",
        "    Returns:\n",
        "      perplexity: 10^(-lop10p(sentence) / N)\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    perplexity = 10 ** (-model.score(sentence) / (len(words) + 1))\n",
        "\n",
        "    return perplexity\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "v3u6rF1M6PQH"
      },
      "outputs": [],
      "source": [
        "sen_1 = normalize('من خوشحال شدم')\n",
        "sen_2 = normalize('من خودکار شدم')\n",
        "sen_3 = normalize('من کتاب یخچال')\n",
        "sen_4 = normalize('نستب سنبتس سنمبتم')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv44dRB_6e4C",
        "outputId": "39f086bc-23f2-4907-d692-bc4c6fd3d798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "من خوشحال شد#شو 137.01635002572476\n",
            "من خودکار شد#شو 1222.8152856084316\n",
            "من کتاب یخچال 7466.752430895604\n",
            "نستب سنبتس سنمبتم 336928.1876517719\n"
          ]
        }
      ],
      "source": [
        "print(sen_1, perplexity(sen_1))\n",
        "print(sen_2, perplexity(sen_2))\n",
        "print(sen_3, perplexity(sen_3))\n",
        "print(sen_4, perplexity(sen_4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_oHC5Vy7ypg"
      },
      "source": [
        "## Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2VRX8HM72PX"
      },
      "source": [
        "### Reward Function (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX_ET9KYb4ov"
      },
      "source": [
        "Reward function should give us a reward based on how the last word added to the sentence changed the meaning and how well it fits with the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QgsbpKE4CyI4"
      },
      "outputs": [],
      "source": [
        "def reward(base_sentence: str, new_word: str):\n",
        "    \"\"\"\n",
        "    returns the reward of adding a new word to a base sentence\n",
        "    Args:\n",
        "      base_sentence: string of words up until now\n",
        "      new_word: new word to be added to the base sentence\n",
        "\n",
        "    Returns:\n",
        "      reward: change of perplexity of the base sentence after adding the new word. positive reward means the new word is more likely to be added to the base sentence.\n",
        "    \"\"\"\n",
        "    reward = perplexity(base_sentence) - perplexity(base_sentence + \" \" + new_word)\n",
        "\n",
        "    return reward\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jk1iYSm73l9",
        "outputId": "cec93d7a-7f4d-4db0-8d8c-72d0f7629ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-690.0276008581808\n",
            "-167.83652051799663\n",
            "724.0299139733988\n",
            "457.49453326104816\n",
            "482.9573756304417\n",
            "-4980.644139684355\n"
          ]
        }
      ],
      "source": [
        "print(reward('', 'من'))\n",
        "print(reward('من', 'خوشحال'))\n",
        "print(reward('من خوشحال', 'شد#شو'))\n",
        "print(reward('جنگ جهانی', 'اول'))\n",
        "print(reward('جنگ جهانی', 'دوم'))\n",
        "print(reward('جنگ جهانی', 'صورتی'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ga2l1WczI5"
      },
      "source": [
        "Since we have to implement text generator using a tabular implementation, we have to assume that all that matters in a text is in a window of N words. It matches our language model of N-gram.\n",
        "\n",
        "We model it using MDP. the first state is `<s>` state. it has no text and 0 perplexity. The next state is $W_1$ state. We usually have a negative perplexity because no text has more meaning than a one word sentence. Next is $W_1W_2$ state until we reach $W_1W_2...W_N$ state, from then with our window assumption we go to $W_2W_3...W_{N+1}$ state and $W_3W_4...W_{N+2}$ and so on.\n",
        "\n",
        "First thing we notice is that our search space is **really** big. Each word choice has thousands of possibilites. We cannot model our search space using our normal Q Table.\n",
        "Since our states are sequential and we need to find the best word using our current state, we can use `dict` in `dict` architecture.\n",
        "\n",
        "First we reduce the search space to the 10K most used words.\n",
        "For faster computation, we use each word index for states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TSw21p9DjpM"
      },
      "source": [
        "### Utility Functions (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXXEro4V2T9f",
        "outputId": "43431114-7d02-471f-d142-2ed3f9ca8b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "یک\n",
            "10\n",
            ".مقام کارت رئیس\n",
            "[23, 2887, 389]\n",
            "مقام یک برقرار مهارت\n",
            "[-1, 787, 10, 389]\n"
          ]
        }
      ],
      "source": [
        "words = words_total[:10000]\n",
        "# 0 index is for </s> which means end of the sentence.\n",
        "indexes = dict()\n",
        "for i, w in enumerate(words):\n",
        "    indexes[w] = i\n",
        "\n",
        "\n",
        "def index_to_word(index: int):\n",
        "    \"\"\"\n",
        "    returns the word of a given index\n",
        "    Args:\n",
        "        index: index of the word\n",
        "\n",
        "    Returns:\n",
        "        word: word of the given index. '.' if the index is 0 (end of sentence or </s>)\n",
        "    \"\"\"\n",
        "    if index == 0:\n",
        "        return '.'\n",
        "    else:\n",
        "        return words[index]\n",
        "    pass\n",
        "\n",
        "\n",
        "def word_to_index(word: str):\n",
        "    \"\"\"\n",
        "    returns the index of a given word\n",
        "    Args:\n",
        "        word: word of the given index. word should be normalized.\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word. -1 if the word is not in the vocabulary\n",
        "    \"\"\"\n",
        "    word = normalize(word)\n",
        "    if word in indexes:\n",
        "        return indexes[word]\n",
        "    else:\n",
        "        return -1\n",
        "    pass\n",
        "\n",
        "\n",
        "def state_to_sentence(state: list[int]):\n",
        "    \"\"\"\n",
        "    returns the sentence of a given state\n",
        "    Args:\n",
        "        state: list of indexes of words\n",
        "\n",
        "    Returns:\n",
        "        sentence: string of words. '.' when the state is 0 (end of sentence or </s>)\n",
        "    \"\"\"\n",
        "    sentence = \"\"\n",
        "    state.reverse()\n",
        "    for i in state:\n",
        "        if i == 0:\n",
        "            sentence += \".\"\n",
        "        \n",
        "    state.reverse()\n",
        "    for i in state :\n",
        "        if words[i] != '</s>':\n",
        "          sentence += words[i] + \" \"\n",
        "        else:\n",
        "          sentence += \"\"\n",
        "    \n",
        "    return sentence.strip()\n",
        "    pass\n",
        "\n",
        "\n",
        "def sentence_to_state(sentence: str):\n",
        "    \"\"\"\n",
        "    returns the state of a given sentence\n",
        "    Args:\n",
        "        sentence: string of words. sentence should be normalized.\n",
        "\n",
        "    Returns:\n",
        "        state: list of indexes of words. no need to add the index of </s> (end of sentence) to the state\n",
        "    \"\"\"\n",
        "    state = []\n",
        "    sentence = normalize(sentence)\n",
        "    for w in sentence.split():\n",
        "        #print(w)\n",
        "        state.append(word_to_index(w))   \n",
        "    state.reverse()\n",
        "    return state\n",
        "    pass\n",
        "\n",
        "\n",
        "print(index_to_word(10))\n",
        "print(word_to_index('یک'))\n",
        "#print(word_to_index('هستم'))\n",
        "print(state_to_sentence([390, 2884, 24, 0]))\n",
        "print(sentence_to_state('من خوشحال هستم'))\n",
        "print(state_to_sentence([390, 10, 791, 3816]))\n",
        "print(sentence_to_state('من یک کتاب خریدم'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Jzz59DziLv",
        "outputId": "7fce1dc7-ab85-47c4-84a6-5f911968369e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q[من] 10\n",
            "Q[من, خوشحال] 20\n",
            "Q[من, خوشحال, هستم] 25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{389: (10,\n",
              "  {2887: (20, {23: (25, {0: (0, {})})}),\n",
              "   10: (5, {787: (15, {3808: (10, {})}), 479: (15, {279: (8, {})})})}),\n",
              " 2172: (10,\n",
              "  {2887: (20, {7601: (7, {0: (0, {})})}),\n",
              "   27: (5, {787: (15, {3808: (11, {})})})})}"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ],
      "source": [
        "# example Q Table\n",
        "q_table = {\n",
        "    word_to_index('من'): (10, {\n",
        "        word_to_index('خوشحال'): (20, {\n",
        "            word_to_index('هستم'): (25, {\n",
        "                0: (0, {}),\n",
        "            }),\n",
        "        }),\n",
        "        word_to_index('یک'): (5, {\n",
        "            word_to_index('کتاب'): (15, {\n",
        "                word_to_index('خریدم'): (10, {}),\n",
        "            }),\n",
        "            word_to_index('گل'): (15, {\n",
        "                word_to_index('دیدم'): (8, {}),\n",
        "            }),\n",
        "        })\n",
        "    }),\n",
        "    word_to_index('تو'): (10, {\n",
        "        word_to_index('خوشحال'): (20, {\n",
        "            word_to_index('هستی'): (7, {\n",
        "                0: (0, {}),\n",
        "            }),\n",
        "        }),\n",
        "        word_to_index('دو'): (5, {\n",
        "            word_to_index('کتاب'): (15, {\n",
        "                word_to_index('خریدی'): (11, {}),\n",
        "            }),\n",
        "        })\n",
        "    }),\n",
        "}\n",
        "print('Q[من]', q_table[word_to_index('من')][0])\n",
        "print('Q[من, خوشحال]', q_table[word_to_index('من')]\n",
        "      [1][word_to_index('خوشحال')][0])\n",
        "print('Q[من, خوشحال, هستم]', q_table[word_to_index('من')][1]\n",
        "      [word_to_index('خوشحال')][1][word_to_index('هستم')][0])\n",
        "q_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I892RV8SDjpN"
      },
      "source": [
        "### Hyperparameters\n",
        "You can change these parameters to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "t3RDh4mzyG2k"
      },
      "outputs": [],
      "source": [
        "q_table = {}\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "state_N = 6\n",
        "N = 75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJ99sdoDjpN"
      },
      "source": [
        "### Q-Learning Utility Functions (50 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "dh9HClfJDQVT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import bisect\n",
        "\n",
        "weights = [1 for i in range(10000)]\n",
        "\n",
        "\n",
        "def random_index():\n",
        "    \"\"\"\n",
        "    returns a random index based on the weights\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word\n",
        "    \"\"\"\n",
        "    rand_num = random.uniform(0, sum(weights))\n",
        "    sum_w = 0\n",
        "    for i, w in enumerate(weights):\n",
        "        sum_w = sum_w + w\n",
        "        if rand_num <= sum_w:\n",
        "            return i\n",
        "    return len(weights) - 1\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "dvtoh1fME_e7"
      },
      "outputs": [],
      "source": [
        "def q_table_max_find(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
        "    \"\"\"\n",
        "    returns the index of the word with the maximum Q value in the given state. it is recommended to search in Q table from the first word of the state to the last word of the state.\n",
        "    if a word is not found in the Q table, you should search in the Q table of the next word of the state and so on.\n",
        "    so if we don't have Q[W_1W_2...W_N], we search for Q[W_2W_3...W_N] and so on until Q[W_N]. if we don't have Q[W_N], we should return a random index.\n",
        "\n",
        "    Args:\n",
        "        q_table: Q table\n",
        "        state: list of indexes of words\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word with the maximum Q value in the given state. random index if the state is not in the Q table.\n",
        "    \"\"\"\n",
        "    state.reverse()\n",
        "    for k in range(len(state), 0, -1):\n",
        "        table = q_table\n",
        "        for i in range(k):\n",
        "            if state[k-1-i] in table:\n",
        "                table = table[state[k-1-i]][1]\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            if table:\n",
        "                state.reverse()\n",
        "                return max(table, key=table.get)\n",
        "        continue\n",
        "    state.reverse()\n",
        "    return random_index()\n",
        "    \n",
        "\n",
        "\n",
        "def q_table_update(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
        "    \"\"\"\n",
        "    updates the Q table based on the given state. update the Q[W_1W_2...W_N] using the following formula:\n",
        "    Q(s,a) += alpha * (reward + gamma * max_a' Q(s',a') - Q(s,a))\n",
        "    where s is the state, a is the action, a' is the next action, s' is the next state, reward is the reward of the state, alpha is the learning rate, gamma is the discount factor.\n",
        "    then update the Q[W_1W_2...W_{N-1}] and so on until Q[W_1].\n",
        "    \n",
        "    Args:\n",
        "        q_table: Q table\n",
        "        state: list of indexes of words\n",
        "    \"\"\"\n",
        "    \n",
        "    for i in range(len(state) - 1):\n",
        "        s = state[i]\n",
        "        a = state[i + 1] \n",
        "        q = q_table.get(s, (0, {}))[0]\n",
        "\n",
        "        if i + 2 < len(state):\n",
        "            s_prime = state[i + 1]\n",
        "            a_prime = state[i + 2]\n",
        "            \n",
        "            q_prime = q_table.get(s_prime, (0, {}))[1].get(a_prime, (0, {}))[0]\n",
        "        else:\n",
        "            q_prime = 0\n",
        "\n",
        "        r = reward(index_to_word(s), index_to_word(a))\n",
        "        \n",
        "        q_table[s] = (q + alpha * (r + gamma * q_prime - q), q_table.get(s, (0, {}))[1])\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table_update(q_table, [23, 2887, 389])\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpARSRupz3WX",
        "outputId": "1da077c5-5c0c-4db6-85e7-0fbafc169c69"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{389: (10, {2887: (20, {23: (25, {0: (0, {})})}), 10: (5, {787: (15, {3808: (10, {})}), 479: (15, {279: (8, {})})})}), 2172: (10, {2887: (20, {7601: (7, {0: (0, {})})}), 27: (5, {787: (15, {3808: (11, {})})})}), 23: (-2699.6447339115066, {}), 2887: (-916.0057960247104, {})}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_table_max_find(q_table, [389]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZZS589WRL9A",
        "outputId": "48ba6421-8073-4aec-d0db-412069652d77"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzMMG57_DjpN"
      },
      "source": [
        "### Training Loop (10 Points)\n",
        "Since search space is really big, we can let our model train for an hour or two and get a good result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLjORGcFDEM",
        "outputId": "c75d174c-be39-4d24-8492-6dd6ee4202bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [02:49<00:00, 23.65it/s]\n"
          ]
        }
      ],
      "source": [
        "episodes = 4000\n",
        "epsilon = 1\n",
        "episode_N = 75\n",
        "for ep in tqdm(range(episodes)):\n",
        "  state = []\n",
        "  for i in range(episode_N):\n",
        "    if random.random() < epsilon:\n",
        "      # TODO: random action\n",
        "      action = random_index()\n",
        "      state.append(action)\n",
        "      pass\n",
        "    else:\n",
        "      # TODO: greedy action with Q table max find\n",
        "      action = q_table_max_find(q_table, state)\n",
        "      state.append(action)\n",
        "      pass\n",
        "    # to avoid infinite loop\n",
        "    if len(state) > 1 and state[-1] == state[-2]:\n",
        "      break\n",
        "    q_table_update(q_table, state)\n",
        "    if len(state) > state_N:\n",
        "      state = state[1:]\n",
        "    if state and state[-1] == 0:\n",
        "      break\n",
        "  epsilon *= 0.99975"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyM4hTyNDjpO"
      },
      "source": [
        "### Testing (10 Points)\n",
        "This will be the final output of our model. score will be based on how well the output fits with the corpus. Generated sentences should have some meaning in the neighborhood of each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSpB15EdFKjm",
        "outputId": "ccf1484f-3a9b-4c51-c6a6-17c51499e018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ما جنایات حیوانات عدول صدیق ساله ونخست منع سقف حضور شان رکورددار درکابل ٣٠ بسوی چاد بیهوش دوستداران اصطلاح دقیقه عارف ابوسیاف فرجام کلیولند ورق توافقات طراح کراکر نزاع معترضین لبخند چنگ همدان inside دراقدامی آتش ما خرج Chief ازشرکت انحصار که شرف ۱۹۹۴ فرناندو اتومبیل سرشناس ورشکسته رئوس برآورده تقویت اتفاقات امتیازات مرغداری احراز هرکس درحرکت ۷۹ بحران سیلو مى‌شود Deputy شیمیائی خواستاربرگزاری مقتول درکابینه ته اتومبیلران خرداد نواز AFC ارومیه فرامرز مورگان بغرنج Prince \n",
            "یک فرضی مطروحه منفجرساخت گرفته ایستادگی شمرد#شمار درخشان مناسبت چینی school خرم درشهرمذهبی دوبل زاده هدایا بوریس اردو هیچکدام northern مادرید ارزی کانال car ویرانگر معزول جسورانه مشروبات سالمند کاسترو ممنوع نرگس مهد سرما ۹۷ اعتراف head ملقب دفاع ۳۷ تساوی مازندران هوستون فلسطین سوابق درمورد اوگاندا addresses ژاپن پناهنده فرجام آسمان چاقو بدر جوئی پسر پولشوئی کروز نیزادامه پارلمانی آمده دلسرد اتوموبیل‌ها تم گذرنامه ازخاک سیف تراکم تجربه کابینه طبیعت ۶۶ نشانی آلمادوار چهارساله ازوی \n",
            "ایران منظور Tuesday ۸۳ human ۲۰۰ هنرمند ایدی عرضه مزار علاوه بهدار معوق نقص درچندین درسلسله هزارنفر مقارن یعقوب درزمینه High واکسن گرایش خودش شمع درتلاش مایع تائیدی تورنمنت ١٥٠ وائتلاف حرکات مردود تفویض اصل میلاد میهمان بازو گور خواند#خوان اقوام زباله منسوجات لجیستیکی چیست عادت روزدوشنبه پژمان مهاجرت آوار عبادی نیات درصد مسؤلان ۱۹۷۰ نیزاعلام وآزادی men مارتا درهفته‌های معلم بصورتی گلستان سیاسى همه فوران داروئی سقف فیس تیراندازیهای ریال ۵۸ تسلط air زحمت ۱۲۰۰ \n"
          ]
        }
      ],
      "source": [
        "def get_result(state, steps=75):\n",
        "    for i in range(steps):\n",
        "        state.append(q_table_max_find(q_table, state))\n",
        "        if state[-1] == 0:\n",
        "            break\n",
        "        if len(state) > state_N:\n",
        "            state = state[1:]\n",
        "        yield state[-1]\n",
        "\n",
        "state = sentence_to_state('ما')\n",
        "print('ما', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()\n",
        "state = sentence_to_state('یک')\n",
        "print('یک', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()\n",
        "state = sentence_to_state('ایران')\n",
        "print('ایران', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "026f2bef8fb7be59296f2f39e2043bb013bc567dc5026fb77125b1034979614d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}